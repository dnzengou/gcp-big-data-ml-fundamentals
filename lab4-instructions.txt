Overview
In this lab, you own a fleet of New York City taxi cabs and are looking to monitor how well your business is doing in real-time. You will build a streaming data pipeline to capture taxi revenue, passenger count, ride status, and much more and visualize the results in a management dashboard.

Set up your environments
Qwiklabs setup
For each lab, you get a new GCP project and set of resources for a fixed time at no cost.

Make sure you signed into Qwiklabs using an incognito window.

Note the lab's access time (for example, img/time.png and make sure you can finish in that time block.

There is no pause feature. You can restart if needed, but you have to start at the beginning.

When ready, click img/start_lab.png.

Note your lab credentials. You will use them to sign in to Cloud Platform Console. img/open_google_console.png

Click Open Google Console.

Click Use another account and copy/paste credentials for this lab into the prompts.

If you use other credentials, you'll get errors or incur charges.

Accept the terms and skip the recovery resource page.
Do not click End Lab unless you are finished with the lab or want to restart it. This clears your work and removes the project.

Check project permissions
Before you begin your work on Google Cloud, you need to ensure that your project has the correct permissions within Identity and Access Management (IAM).

In the Google Cloud console, on the Navigation menu (nav-menu.png), click IAM & Admin > IAM.

Confirm that the default compute Service Account {project-number}-compute@developer.gserviceaccount.com is present and has the editor role assigned. The account prefix is the project number, which you can find on Navigation menu > Home.

check-sa.png

If the account is not present in IAM or does not have the editor role, follow the steps below to assign the required role.

In the Google Cloud console, on the Navigation menu, click Home.

Copy the project number (e.g. 729328892908).

On the Navigation menu, click IAM & Admin > IAM.

At the top of the IAM page, click Add.

For New members, type:

{project-number}-compute@developer.gserviceaccount.com
Replace {project-number} with your project number.

For Role, select Project > Editor. Click Save.
add-sa.png

Note your project name; confirm that needed APIs are enabled
Make a note of the name of your Google Cloud project. This value is shown in the top bar of the Cloud Console.

In the Cloud Console, in the Navigation menu, click Home.

In the Project Info section, copy and save your Project ID value for later use. Your project ID will resemble qwiklabs-gcp-d2e509fed105b3ed.

In the Cloud Console, in the Navigation menu, click APIs & services.

Console_APIs.png

Scroll down in the list of enabled APIs, and confirm that these APIs are enabled:

Cloud Pub/Sub API
Dataflow API
If one or more API is not enabled, click the Enable APIs and services button at the top. Search for the APIs by name and enable each API for your current project.

Task 1. Create a Pub/Sub topic and BigQuery dataset
Pub/Sub is an asynchronous global messaging service. By decoupling senders and receivers, it allows for secure and highly available communication between independently written applications. Pub/Sub delivers low-latency, durable messaging.

In Pub/Sub, publisher applications and subscriber applications connect with one another through the use of a shared string called a topic. A publisher application creates and sends messages to a topic. Subscriber applications create a subscription to a topic to receive messages from it.

Google maintains a few public Pub/Sub streaming data topics for labs like this one. We'll be using the NYC Taxi & Limousine Commissionâ€™s open dataset.

BigQuery is a serverless data warehouse. Tables in BigQuery are organized into datasets. In this lab, messages published into Pub/Sub will be aggregated and stored in BigQuery.

To create a new BigQuery dataset:

Option 1: The command-line tool
Open Cloud Shell and run the below command to create the taxirides dataset.

bq mk taxirides
Run this command to create the taxirides.realtime table (empty schema that you will stream into later).

bq mk \
--time_partitioning_field timestamp \
--schema ride_id:string,point_idx:integer,latitude:float,longitude:float,\
timestamp:timestamp,meter_reading:float,meter_increment:float,ride_status:string,\
passenger_count:integer -t taxirides.realtime
Option 2: The BigQuery Console UI
Skip these steps if you created the tables using the command line.

In the Cloud Console, go to Navigation menu > BigQuery.

Once there, click on your Project ID from the left-hand menu.

Now on the right-hand side of the Cloud Console, underneath the query editor, click Create dataset.

Give the new dataset the name taxirides, leave all the other fields the way they are, and click Create dataset.

If you look at the left-hand resources menu, you should see your newly created dataset.

Click on the taxirides dataset.

Click create table.

Name the table realtime

For the schema, click edit as text and paste in the below:

ride_id:string,
point_idx:integer,
latitude:float,
longitude:float,
timestamp:timestamp,
meter_reading:float,
meter_increment:float,
ride_status:string,
passenger_count:integer
Under Partition and cluster settings, select the timestamp option for the Partitioning field.

Confirm against the below screenshot:

bq-taxi-table.png

Click the Create table button.
Task 2. Create a Cloud Storage bucket
Skip this step if you already have a bucket created.

Cloud Storage allows world-wide storage and retrieval of any amount of data at any time. You can use Cloud Storage for a range of scenarios including serving website content, storing data for archival and disaster recovery, or distributing large data objects to users via direct download. In this lab, you use Cloud Storage to provide working space for your Dataflow pipeline.

In the Cloud Console, go to Navigation menu > Storage.
Click Create bucket.
For Name, paste in your Project ID.
For Default storage class, click Multi-regional if it is not already selected.
For Location, choose the selection closest to you.
Click Create.
Task 3. Set up a Dataflow Pipeline
Dataflow is a serverless way to carry out data analysis. In this lab, you set up a streaming data pipeline to read sensor data from Pub/Sub, compute the maximum temperature within a time window, and write this out to BigQuery.

In the Cloud Console, go to Navigation menu > Dataflow.

In the top menu bar, click Create job from template.

Enter streaming-taxi-pipeline as the Job name for your Dataflow job.

Under Dataflow template, select the Pub/Sub Topic to BigQuery template.

Under Input Pub/Sub topic, enter projects/pubsub-public-data/topics/taxirides-realtime

Under BigQuery output table, enter <myprojectid>:taxirides.realtime

Note: There is a colon : between the project and dataset name and a dot . between the dataset and table name

Under Temporary location, enter gs://<mybucket>/tmp/

dataflow.png

Click the Run Job button.
A new streaming job has started! You can now see a visual representation of the data pipeline.

Task 4. Analyze the taxi data using BigQuery
To analyze the data as it is streaming:

In the Cloud Console, open the Navigation menu and select BigQuery.

Enter the following query in the Query editor and click Run:

SELECT * FROM taxirides.realtime LIMIT 10
If no records are returned, wait another minute and re-run the above query (Dataflow takes 3-5 minutes to setup the stream). You will receive a similar output:
taxi-bq-output.png

Task 5. Perform aggregations on the stream for reporting
Copy and paste the below query and click Run.

WITH streaming_data AS (

SELECT
  timestamp,
  TIMESTAMP_TRUNC(timestamp, HOUR, 'UTC') AS hour,
  TIMESTAMP_TRUNC(timestamp, MINUTE, 'UTC') AS minute,
  TIMESTAMP_TRUNC(timestamp, SECOND, 'UTC') AS second,
  ride_id,
  latitude,
  longitude,
  meter_reading,
  ride_status,
  passenger_count
FROM
  taxirides.realtime
WHERE ride_status = 'dropoff'
ORDER BY timestamp DESC
LIMIT 100000

)

# calculate aggregations on stream for reporting:
SELECT
 ROW_NUMBER() OVER() AS dashboard_sort,
 minute,
 COUNT(DISTINCT ride_id) AS total_rides,
 SUM(meter_reading) AS total_revenue,
 SUM(passenger_count) AS total_passengers
FROM streaming_data
GROUP BY minute, timestamp
The result shows key metrics by the minute for every taxi drop-off.

Task 6. Create a real-time dashboard
Open this Google Data Studio link in a new incognito browser tab.

On the Reports page, in the Start with a Template section, click the [+] Blank Report template.

3e9763840c2a2303.png

If prompted with the Welcome to Google Studio window, click Get started.

Check the checkbox to acknowledge the Google Data Studio Additional Terms, and click Accept.

Select No thanks to all 4 questions, then click Done.

Switch back to the BigQuery Console.

Click Explore Data > Explore with Data Studio in BigQuery page.

Click Get Started, then click Authorize.

Specify the below settings:

Chart type: Combo chart
Date range Dimension: dashboard_sort
Dimension: dashboard_sort
Drill Down: dashboard_sort (Make sure that Drill down option is turned ON)
Metric: SUM() total_rides, SUM() total_passengers, SUM() total_revenue
Sort: dashboard_sort, Ascending (latest rides first)
combochart.png

Note: Visualizing data at a minute-level granularity is currently not supported in Data Studio as a timestamp. This is why we created our own dashboard_sort dimension.
When you're happy with your dashboard, click Save to save this data source.

Whenever anyone visits your dashboard, it will be up-to-date with the latest transactions. You can try it yourself by clicking on the Refresh button near the Save button.

Task 7. Create a time series dashboard
Click this Google Data Studio link to open Data Studio in a new browser tab.

On the Reports page, in the Start with a Template section, click the [+] Blank Report template.

3e9763840c2a2303.png

A new, empty report opens with Add data to report.
connectdata.png

From the list of Google Connectors, select the BigQuery tile.

Under Custom query, click qwiklabs-gcp-xxxxxxx > Enter Custom Query, add the following query.

SELECT
  *
FROM
  taxirides.realtime
WHERE
  ride_status='dropoff'
customquery.png

Click Add > Add to Report.

Create a time series chart
In the Data panel, scroll down to the bottom right and click Add a Field section. Click All Fields on the left corner.

Change the field timestamp type to Date & Time > Date Hour Minute (YYYYMMDDhhmm).

Click Done.

Click Add a chart.

Choose Time series chart.

Position the chart in the bottom left corner - in the blank space.

In the Data panel on the right, change the following:

Dimension: timestamp
Metric: meter_reading(SUM)
Your time series chart should look similar to this:

timeseries.png

Task 8. Stop the Dataflow job
Navigate back to Dataflow.

Click the streaming-taxi-pipeline.

Click Stop and select Cancel > Stop Job.

This will free up resources for your project.

Congratulations!
In this lab you used Pub/Sub to collect streaming data messages from taxis and feed it through your Dataflow pipeline into BigQuery.

